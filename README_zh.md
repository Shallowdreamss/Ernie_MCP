# ERNIE-MCP é¡¹ç›®å¿«é€Ÿå…¥é—¨æŒ‡å—

## ğŸ“– æ•™ç¨‹æ¦‚è§ˆ

æœ¬é¡¹ç›®ä¸ºåŠ©åŠ›æ–‡å¿ƒå¼€æºæ‰€æ’°å†™ï¼Œé€šè¿‡FastDeployå·¥å…·åŒ…å®ç°**ERNIE-4.5-21B-A3B**æ¨¡å‹æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆï¼ŒåŠ©åŠ›å¼€å‘è€…å¿«é€ŸæŒæ¡ERNIE 4.5ç³»åˆ—æ¨¡å‹çš„æœ¬åœ°åŒ–éƒ¨ç½²åŠMCPæœåŠ¡æ¥å…¥ï¼Œä½“éªŒMCPæŸ¥è¯¢å®æ—¶å¤©æ°”ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python**: 3.12
- **PaddlePaddle**: æœ€æ–°ç‰ˆæœ¬ (GPUç‰ˆæœ¬)
- **ERNIEKit**: æœ€æ–°ç‰ˆæœ¬
- **GPU**: æ¨èå•å¼  80GB A/H ç³»åˆ—GPUï¼Œæœ€ä½ 24GB æ˜¾å­˜
- **ç³»ç»Ÿ**: Linux/Windows/macOS

æœ¬é¡¹ç›®åœ¨å•å¼ A800ä¸Šå¼€å‘

### ç¯å¢ƒé…ç½®

æ‰“å¼€ä¸€ä¸ªç»ˆç«¯ï¼Œåœ¨æ ¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒï¼ˆå› ä¸ºé¡¹ç›®ä¾èµ–é«˜ç‰ˆæœ¬pythonï¼Œæ‰€ä»¥æ­¤å¤„å®‰è£…python==3.12ï¼‰ï¼š

```
conda create -p /home/aistudio/envs/mcp python=3.12
```

ä¸€é”®å¯åŠ¨è™šæ‹Ÿç¯å¢ƒï¼š

```
source activate /home/aistudio/envs/mcp/
```

å®‰è£…è¯¥é¡¹ç›®ç›¸å…³ä¾èµ–ï¼š

```
pip install mcp httpx openai python-dotenv
```

åç»­å¯åŠ¨ MCP æœåŠ¡æ—¶éœ€è¦ uv åŒ…ç®¡ç†å·¥å…·ï¼Œä¸ºäº†æå‡å®‰è£…é€Ÿåº¦ï¼Œå¯ä»¥é…ç½®é•œåƒ

æ–¹æ³•ï¼šè¿›å…¥ Conda ç¯å¢ƒåï¼Œä¿®æ”¹æ¿€æ´»è„šæœ¬ï¼ˆå¦‚ `~/.bashrc` æˆ– `~/.zshrc`ï¼‰

```
echo 'export UV_INDEX_URL="https://pypi.tuna.tsinghua.edu.cn/simple"' >> ~/.bashrc

source ~/.bashrc  # é‡æ–°åŠ è½½é…ç½®
```

```
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-username/Ernie_MCP.git
cd Ernie_MCP

# å®‰è£… PaddlePaddle GPU ç‰ˆæœ¬
pip install paddlepaddle-gpu==3.1.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/

# å…‹éš† ERNIEKit ä»“åº“
git clone https://github.com/PaddlePaddle/ERNIE.git -b develop

# å®‰è£… ERNIEKit ä¾èµ–
cd ERNIE
pip install -r requirements/gpu/requirements.txt

# é¦–å…ˆè¯·å…ˆå®‰è£…aistudio-sdkåº“
pip install --upgrade aistudio-sdk
# ä½¿ç”¨aistudio cliä¸‹è½½æ¨¡å‹ï¼Œ0.3Bçš„æ¨¡å‹å¯¹è¯­æ„çš„ç†è§£è²Œä¼¼ä¸æ˜¯å¾ˆå¼ºï¼Œæ‰€ä»¥æˆ‘ä»¬é€‰æ‹©21Bæ¨¡å‹
aistudio download --model PaddlePaddle/ERNIE-4.5-21B-A3B-Paddle --local_dir baidu/ERNIE-4.5-21B-A3B-Paddle

# å®‰è£…FastDeploy
python -m pip install fastdeploy-gpu -i https://www.paddlepaddle.org.cn/packages/stable/fastdeploy-gpu-80_90/ --extra-index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple
```

### å¯åŠ¨Fastdeploy

```
# ç»ˆç«¯è¿è¡Œ
python -m fastdeploy.entrypoints.openai.api_server \
       --model baidu/ERNIE-4.5-21B-A3B-Paddle \
       --port 8180 \
       --metrics-port 8181 \
       --engine-worker-queue-port 8182 \
       --max-model-len 32768 \
       --max-num-seqs 32
```

**æˆ–**  è¿è¡Œä¸‹é¢çš„ä»£ç å—

```
#  FastDeployå®Œæ•´å¯åŠ¨ä»£ç 
import subprocess
import time
import requests
import threading

def start_fastdeploy():
    cmd = [
        "python", "-m", "fastdeploy.entrypoints.openai.api_server",
        "--model", "baidu/ERNIE-4.5-21B-A3B-Paddle",
        "--port", "8180",
        "--metrics-port", "8181", 
        "--engine-worker-queue-port", "8182",
        "--max-model-len", "32768",
        "--max-num-seqs", "32"
    ]
    
    print("ğŸš€ å¯åŠ¨FastDeployæœåŠ¡...")
    print("-" * 50)
    
    process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        universal_newlines=True,
        bufsize=1
    )
    
    print(f"ğŸ“ PID: {process.pid}")
    
    service_ready = False
    
    def monitor_logs():
        nonlocal service_ready
        try:
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    line = output.strip()
                    print(f"[æ—¥å¿—] {line}")
                    
                    if "Loading Weights:" in line and "100%" in line:
                        print("âœ… æƒé‡åŠ è½½å®Œæˆ")
                    elif "Loading Layers:" in line and "100%" in line:
                        print("âœ… å±‚åŠ è½½å®Œæˆ")
                    elif "Worker processes are launched" in line:
                        print("âœ… å·¥ä½œè¿›ç¨‹å¯åŠ¨")
                    elif "Uvicorn running on" in line:
                        print("ğŸ‰ æœåŠ¡å¯åŠ¨å®Œæˆï¼")
                        service_ready = True
                        break
        except Exception as e:
            print(f"æ—¥å¿—ç›‘æ§é”™è¯¯: {e}")
    
    log_thread = threading.Thread(target=monitor_logs, daemon=True)
    log_thread.start()
    
    start_time = time.time()
    while time.time() - start_time < 120:
        if service_ready:
            break
        if process.poll() is not None:
            print("âŒ è¿›ç¨‹é€€å‡º")
            return None
        time.sleep(1)
    
    if not service_ready:
        print("âŒ å¯åŠ¨è¶…æ—¶")
        process.terminate()
        return None
    
    print("-" * 50)
    return process

def test_model():
    try:
        import openai
        
        print("ğŸ”Œ æµ‹è¯•æ¨¡å‹è¿æ¥...")
        
        client = openai.Client(base_url="http://localhost:8180/v1", api_key="null")
        
        response = client.chat.completions.create(
            model="null",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": "ä½ å¥½"}
            ],
            max_tokens=50,
            stream=False
        )
        
        print("âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
        print(f"ğŸ¤– å›å¤: {response.choices[0].message.content}")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def check_service():
    try:
        response = requests.get("http://localhost:8180/v1/models", timeout=3)
        return response.status_code == 200
    except:
        return False

def setup_service():

    print("=== ERNIE-4.5-21B-A3B-Paddle æœåŠ¡å¯åŠ¨ ===")
    
    if check_service():
        print("âœ… å‘ç°è¿è¡Œä¸­çš„æœåŠ¡")
        if test_model():
            print("ğŸ‰ æœåŠ¡å·²å°±ç»ªï¼")
            return True
        print("âš ï¸ æœåŠ¡å¼‚å¸¸ï¼Œé‡æ–°å¯åŠ¨")
    
    process = start_fastdeploy()
    
    if process is None:
        print("âŒ å¯åŠ¨å¤±è´¥")
        return False
    
    if test_model():
        print("ğŸŠ å¯åŠ¨æˆåŠŸï¼ç°åœ¨å¯ä»¥è¿è¡ŒçŸ¥è¯†å›¾è°±ä»£ç ")
        return True
    else:
        print("âŒ å¯åŠ¨ä½†è¿æ¥å¤±è´¥")
        return False

if __name__ == "__main__" or True:
    setup_service()
```

### æµ‹è¯•æ¨¡å‹

è¿è¡Œä¸‹é¢çš„ä»£ç å—æˆ–è€…åœ¨ç»ˆç«¯è¿è¡Œ

```
python model_test.py
```

```
# ä»£ç å—
import openai
host = "0.0.0.0"
port = "8180"
client = openai.Client(base_url=f"http://{host}:{port}/v1", api_key="null")

response = client.chat.completions.create(
    model="null",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹."},
        {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹åŒ—äº¬"},
    ],
    stream=True,
)
for chunk in response:
    if chunk.choices[0].delta:
        print(chunk.choices[0].delta.content, end='')
print('\n')
```

## ğŸš€å¿«é€Ÿä¸Šæ‰‹

### ä»£ç è¿è¡Œ

åœ¨ç»ˆç«¯ä¸­è¿è¡Œ

```
python Weather.py Ernie_Server.py
```

æ•ˆæœå±•ç¤º
![](https://ai-studio-static-online.cdn.bcebos.com/99d4751a50a843da99b1275b8a3a2be167925ddae6cb4ea89696de40ffb5ef94)
```
python Weather_zh.py Ernie_Server_zh.py
```
![](https://ai-studio-static-online.cdn.bcebos.com/31a325fc4fbb4fd8b2fd3fe7fa149c79515ba5125df64d68b32ad86437baa637)
![](https://ai-studio-static-online.cdn.bcebos.com/9f8f60ea41fb433da0306126370c21afd32757d5ecac452984dc3171c45a26c3)

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼æ— è®ºæ˜¯å‘ç°bugã€æå‡ºæ”¹è¿›å»ºè®®ï¼Œè¿˜æ˜¯è´¡çŒ®æ–°çš„æ•™ç¨‹å†…å®¹ã€‚

### è´¡çŒ®æ–¹å¼

- ğŸ› **Bug æŠ¥å‘Š**ï¼š å‘ç°é—®é¢˜è¯·æäº¤ [Issue](https://github.com/G-Fuji/ERNIE-Tutorial/issues)
- ğŸ’¡ **åŠŸèƒ½å»ºè®®**ï¼š æœ‰å¥½æƒ³æ³•è¯·åœ¨ [Discussions](https://github.com/G-Fuji/ERNIE-Tutorial/discussions) ä¸­è®¨è®º
- ğŸ“ **æ–‡æ¡£æ”¹è¿›**ï¼š å¸®åŠ©å®Œå–„æ•™ç¨‹å†…å®¹å’Œä»£ç æ³¨é‡Š
- ğŸ”§ **ä»£ç è´¡çŒ®**ï¼š æäº¤ Pull Request è´¡çŒ®æ–°åŠŸèƒ½æˆ–ä¿®å¤
- ğŸ“ **æ•™ç¨‹æ‰©å±•**ï¼š è´¡çŒ®æ–°çš„åº”ç”¨åœºæ™¯å’Œå®æˆ˜æ¡ˆä¾‹

### è´¡çŒ®æµç¨‹

1. Fork æœ¬é¡¹ç›®åˆ°æ‚¨çš„ GitHub è´¦æˆ·
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. è¿›è¡Œæ‚¨çš„ä¿®æ”¹å¹¶ç¡®ä¿ä»£ç è´¨é‡
4. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
5. æ¨é€åˆ°æ‚¨çš„åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
6. å¼€å¯ Pull Request å¹¶è¯¦ç»†æè¿°æ‚¨çš„æ›´æ”¹

### ä»£ç è§„èŒƒ

- éµå¾ª PEP 8 Python ä»£ç è§„èŒƒ
- ä¸ºæ–°åŠŸèƒ½æ·»åŠ é€‚å½“çš„æ³¨é‡Šå’Œæ–‡æ¡£
- ç¡®ä¿æ‰€æœ‰ç¤ºä¾‹ä»£ç éƒ½èƒ½æ­£å¸¸è¿è¡Œ
- æ›´æ–°ç›¸å…³çš„ README å’Œé…ç½®æ–‡ä»¶

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [Apache 2.0 è®¸å¯è¯](https://github.com/G-Fuji/ERNIE-Tutorial/blob/main/LICENSE)ã€‚æ‚¨å¯ä»¥è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘æœ¬é¡¹ç›®çš„ä»£ç ï¼Œä½†è¯·ä¿ç•™åŸå§‹è®¸å¯è¯å£°æ˜ã€‚

## ğŸ“ è”ç³»æˆ‘ä»¬

- **é¡¹ç›®ç»´æŠ¤è€…**ï¼š Ernie_MCP Team
- **æŠ€æœ¯äº¤æµ**ï¼š æ¬¢è¿åœ¨ Issues ä¸­æå‡ºæŠ€æœ¯é—®é¢˜

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹é¡¹ç›®å’Œå›¢é˜Ÿçš„æ”¯æŒï¼š

- [ç™¾åº¦ PaddlePaddle](https://github.com/PaddlePaddle/Paddle) - æ·±åº¦å­¦ä¹ æ¡†æ¶
- [ERNIEKit](https://github.com/PaddlePaddle/ERNIE) - å®˜æ–¹æ¨¡å‹å¼€å‘å¥—ä»¶
- [ERNIE æ¨¡å‹å›¢é˜Ÿ](https://aistudio.baidu.com/modelsoverview) - æä¾›å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹
- æ‰€æœ‰è´¡çŒ®è€…å’Œä½¿ç”¨è€…çš„åé¦ˆä¸æ”¯æŒ

------

â­ **å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼æ‚¨çš„æ”¯æŒæ˜¯æˆ‘ä»¬æŒç»­æ”¹è¿›çš„åŠ¨åŠ›ã€‚**

ğŸš€ **å¼€å§‹æ‚¨çš„ ERNIE å¤§æ¨¡å‹å­¦ä¹ ä¹‹æ—…å§ï¼**

ğŸ”— **ç›¸å…³é“¾æ¥**

- [äººå·¥æ™ºèƒ½å·¥ä½œå®¤](https://aistudio.baidu.com/)
- [PaddlePaddle å®˜ç½‘](https://www.paddlepaddle.org.cn/)
- [ERNIEkit GitHub](https://github.com/PaddlePaddle/ERNIE)



